import ultralytics
import cv2  
from ultralytics import solutions
import torch
import os
from datetime import datetime

ultralytics.checks()

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPU device: {torch.cuda.get_device_name(0)}")

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
screenshots_dir = "detection_screenshots"
os.makedirs(screenshots_dir, exist_ok=True)

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
cap.set(cv2.CAP_PROP_FPS, 30)

assert cap.isOpened(), "Error reading video file"

w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

print(f"üìπ –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {w}x{h}, FPS: {fps}")

video_writer = cv2.VideoWriter("counting.avi",
                               cv2.VideoWriter_fourcc(*"mp4v"),
                               fps, (w, h))

# Define region points
region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]

# Init ObjectCounter
counter = solutions.ObjectCounter(
    show=True,
    region=region_points,
    model="yolov8n.pt",  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ yolov8n.pt
)

print("üé• –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –Ω–∞—á–∞—Ç–∞. –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞.")

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏
last_screenshot_time = 0
screenshot_cooldown = 2  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
screenshot_count = 0

def save_detection_screenshot(frame, detections_count):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è–º–∏"""
    global screenshot_count, last_screenshot_time
    
    current_time = datetime.now()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º cooldown
    if cv2.getTickCount() / cv2.getTickFrequency() - last_screenshot_time < screenshot_cooldown:
        return
    
    screenshot_count += 1
    timestamp = current_time.strftime("%Y%m%d_%H%M%S_%f")[:-3]
    filename = f"{screenshots_dir}/detection_{timestamp}_count_{detections_count}.jpg"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç
    annotated_frame = frame.copy()
    cv2.putText(annotated_frame, f"Detections: {detections_count}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.putText(annotated_frame, current_time.strftime("%Y-%m-%d %H:%M:%S"), (10, 70),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    
    cv2.imwrite(filename, annotated_frame)
    print(f"üì∏ –ê–≤—Ç–æ-—Å–∫—Ä–∏–Ω—à–æ—Ç: {filename}")
    
    last_screenshot_time = cv2.getTickCount() / cv2.getTickFrequency()

# Process video
while cap.isOpened():
    success, im0 = cap.read()
    if not success:
        print("–í–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏–ª–∏ –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è.")
        break
    
    results = counter(im0)
    video_writer.write(results.plot_im)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤ –∑–æ–Ω–µ
    if hasattr(counter, 'in_count') and counter.in_count > 0:
        save_detection_screenshot(im0, counter.in_count)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞–≤–∏—à
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        print("üö™ –í—ã—Ö–æ–¥ –ø–æ –Ω–∞–∂–∞—Ç–∏—é 'q'")
        break

cap.release()
video_writer.release()
cv2.destroyAllWindows()
print(f"‚úÖ –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {screenshot_count}")